# Binary Files Deep Analysis - FHIR Database

**Date**: 2025-10-18
**Database**: fhir_prd_db
**Patient Sample**: e4BwD8ZYDBccepXcJ.Ilo3w3 (C1277724)
**Total Documents Analyzed**: 22,127

---

## üìä Executive Summary

### Document Volume
- **Total DocumentReferences**: 22,127 for single patient
- **Estimated S3 Availability**: ~57% (12,612 documents)
- **Missing from S3**: ~43% (9,515 documents)

### Document Type Distribution (Top 30)

| Document Type | Count | % of Total | Clinical Value |
|---------------|-------|------------|----------------|
| Progress Notes | 7,668 | 34.6% | ‚≠ê‚≠ê‚≠ê HIGH |
| Telephone Encounter | 3,176 | 14.3% | ‚≠ê LOW |
| Assessment & Plan Note | 1,184 | 5.3% | ‚≠ê‚≠ê‚≠ê HIGH |
| Patient Instructions | 856 | 3.9% | ‚≠ê LOW |
| Encounter Summary | 762 | 3.4% | ‚≠ê‚≠ê MEDIUM |
| Nursing Note | 384 | 1.7% | ‚≠ê‚≠ê MEDIUM |
| (blank) | 336 | 1.5% | ‚ùå UNKNOWN |
| PT Bill of Right Form FDT | 318 | 1.4% | ‚ùå NON-CLINICAL |
| Important Notice Regarding Billing FDT | 317 | 1.4% | ‚ùå NON-CLINICAL |
| CHOP Financial Consent Form | 270 | 1.2% | ‚ùå NON-CLINICAL |
| Auth Treatment & Video Taping ED | 270 | 1.2% | ‚ùå NON-CLINICAL |
| Medicare Outpatients Observation Form FDT | 268 | 1.2% | ‚ùå NON-CLINICAL |
| Advanced Directive Screening FDT | 268 | 1.2% | ‚ùå NON-CLINICAL |
| Anesthesia Preprocedure Evaluation | 200 | 0.9% | ‚≠ê‚≠ê MEDIUM |
| Subjective & Objective | 184 | 0.8% | ‚≠ê‚≠ê MEDIUM |
| Consult Note | 180 | 0.8% | ‚≠ê‚≠ê‚≠ê HIGH |
| Diagnostic imaging study | 163 | 0.7% | ‚≠ê‚≠ê MEDIUM |
| Addendum Note | 152 | 0.7% | ‚≠ê‚≠ê MEDIUM |

**Key Finding**: ~35% of documents are administrative/billing forms with NO clinical value for data extraction.

---

## üìÅ Content Type Distribution

### File Formats Available

| Content Type | Count | % of Total | Extractable? | Processing Method |
|--------------|-------|------------|--------------|-------------------|
| text/html | 7,604 | 34.4% | ‚úÖ YES | HTML parsing ‚Üí text |
| text/rtf | 7,372 | 33.3% | ‚úÖ YES | RTF parsing ‚Üí text |
| (blank/null) | 5,331 | 24.1% | ‚ùå NO | Missing content type |
| application/xml | 763 | 3.4% | ‚úÖ YES | XML parsing |
| application/pdf | 503 | 2.3% | ‚úÖ YES | OCR/PDF text extraction |
| text/xml | 321 | 1.5% | ‚úÖ YES | XML parsing |
| image/tiff | 168 | 0.8% | ‚ö†Ô∏è MAYBE | OCR required |
| image/jpeg | 31 | 0.1% | ‚ö†Ô∏è MAYBE | OCR required |
| video/quicktime | 2 | <0.1% | ‚ùå NO | Video not processable |
| image/png | 1 | <0.1% | ‚ö†Ô∏è MAYBE | OCR required |

**Key Findings**:
- **67.7%** are text-based formats (HTML, RTF) - easily extractable
- **24.1%** have no content type - likely missing from S3
- **2.3%** are PDFs - require specialized extraction
- **0.9%** are images - require OCR

---

## üéØ High-Value Clinical Document Types

Based on the HIGHEST_VALUE_BINARY_DOCUMENT_SELECTION_STRATEGY.md and actual data:

### Priority 1: Diagnosis & Pathology (CRITICAL)
- **Surgical Pathology Reports**: Not explicitly listed by type, likely in "Progress Notes" or unlabeled
- **Expected count**: 2 per patient (diagnosis + recurrence)
- **Variables covered**: diagnosis_date, primary_diagnosis, who_grade, tumor_location, braf_status
- **Search strategy**: Filter by date near surgery dates + keyword matching

### Priority 2: Operative Notes (CRITICAL)
- **Complete Operative Notes**: Not explicitly categorized
- **Expected count**: 2 per patient (per surgery)
- **Variables covered**: surgery_date, surgery_type, surgery_extent, surgery_location
- **Challenge**: Distinguish from "Brief operative notes" or "Anesthesia" notes

### Priority 3: Radiology Reports (HIGH VALUE)
- **Diagnostic imaging study**: 163 documents
- **MRI Reports**: Likely embedded in "Progress Notes" or "Diagnostic imaging study"
- **Expected count**: 5-10 per patient (diagnosis, progression, surveillance)
- **Variables covered**: tumor_size, tumor_location, contrast_enhancement, imaging_findings

### Priority 4: Oncology Notes (HIGH VALUE)
- **Consult Note**: 180 documents
- **Assessment & Plan Note**: 1,184 documents
- **Progress Notes**: 7,668 documents (may include oncology notes)
- **Expected count**: 3-5 per patient (treatment initiations)
- **Variables covered**: chemotherapy_line, chemotherapy_route, chemotherapy_dose, clinical_status

### Priority 5: Molecular Testing Reports (CRITICAL)
- **Not explicitly categorized** - may be in "Progress Notes" or unlabeled
- **Expected count**: 1-2 per patient
- **Variables covered**: braf_status, idh_mutation, mgmt_methylation

---

## üö® Major Challenges Identified

### Challenge 1: Document Type Granularity is Insufficient

**Problem**: High-value documents are NOT clearly labeled in document_type field:
- "Progress Notes" (7,668 docs) contains mix of:
  - Oncology consultation notes ‚≠ê‚≠ê‚≠ê
  - Pathology reports ‚≠ê‚≠ê‚≠ê
  - Radiology reports ‚≠ê‚≠ê‚≠ê
  - Routine follow-up notes ‚≠ê
  - Nursing notes ‚≠ê

**Impact**: Cannot filter high-value documents by document_type alone

**Solution**: Must use **multi-field filtering**:
- `document_type` + `description` + `type_coding_display` + `category_text`
- Temporal filtering (dates near key clinical events)
- Keyword matching in free text

### Challenge 2: 43% of Documents Missing from S3

**Problem**: From empirical testing:
- 148 documents attempted
- 84 extracted successfully (57%)
- 64 failed (43%)

**Impact**: Even if we identify high-value documents in metadata, they may not be retrievable

**Solution**:
- Always verify S3 availability BEFORE adding to extraction queue
- Build fallback document selection for each temporal window
- Implement retry logic with period-to-underscore transformation

### Challenge 3: 35% are Administrative/Billing Documents

**Problem**: Large volume of zero-value documents:
- Billing forms: 1,500+ documents
- Consent forms: 1,000+ documents
- Rights forms: 500+ documents

**Impact**: Noise in document selection, wasted processing resources

**Solution**:
- Exclude document types with "FDT", "Bill", "Consent", "Rights", "Medicare" keywords
- Focus on clinical document types only

### Challenge 4: Mixed Content Types

**Problem**:
- HTML (34.4%) requires HTML parsing
- RTF (33.3%) requires RTF parsing
- PDF (2.3%) requires OCR/PDF extraction
- Images (0.9%) require OCR

**Impact**: Different processing pipelines needed for different content types

**Solution**:
- Build content-type-aware extraction router
- HTML/RTF ‚Üí text extraction (easy)
- PDF ‚Üí PyPDF2 or pdfplumber
- Images ‚Üí OCR (Tesseract or AWS Textract)

---

## üìã Recommended Multiagent Workflow Strategy

### Agent 1: Document Selector (Planning Agent)

**Input**:
- Patient FHIR ID
- Clinical timeline from structured data (v_procedures_tumor, v_medications, v_imaging)

**Process**:
1. Extract key temporal windows (diagnosis, surgeries, treatment starts, progressions)
2. Query v_binary_files with multi-field filters:
   ```sql
   WHERE (
       -- Pathology reports
       (LOWER(dr_type_text) LIKE '%patholog%'
        OR LOWER(dr_description) LIKE '%patholog%'
        OR LOWER(dr_type_coding_displays) LIKE '%patholog%')
       AND dr_date BETWEEN diagnosis_date - INTERVAL '7' DAY
                       AND diagnosis_date + INTERVAL '14' DAY
   )
   OR (
       -- Operative notes
       (LOWER(dr_type_text) LIKE '%operative%'
        OR LOWER(dr_description) LIKE '%operative%')
       AND dr_date BETWEEN surgery_date - INTERVAL '3' DAY
                       AND surgery_date + INTERVAL '3' DAY
   )
   -- ... additional filters for radiology, oncology, molecular
   ```
3. Rank documents by:
   - Temporal proximity to key events
   - Document type priority (pathology > operative > radiology > oncology)
   - Content type extractability (text/html > text/rtf > application/pdf > image)
4. Select top 15-20 documents per patient

**Output**: Prioritized list of document_reference_ids with metadata

---

### Agent 2: S3 Availability Checker

**Input**: List of document_reference_ids from Agent 1

**Process**:
1. For each binary_id:
   ```python
   s3_filename = binary_id.replace('.', '_')
   s3_key = f"prd/source/Binary/{s3_filename}"
   try:
       s3_client.head_object(Bucket=bucket, Key=s3_key)
       return True  # Available
   except NoSuchKey:
       return False  # Missing
   ```
2. Track availability rate
3. If <80% available, trigger fallback document selection

**Output**: Filtered list of AVAILABLE documents

---

### Agent 3: Content Extractor (Execution Agent)

**Input**: List of available binary_ids with content_type

**Process**:
1. Route by content_type:
   - `text/html` ‚Üí BeautifulSoup ‚Üí extract text
   - `text/rtf` ‚Üí striprtf library ‚Üí extract text
   - `application/pdf` ‚Üí pdfplumber ‚Üí extract text
   - `application/xml` or `text/xml` ‚Üí ElementTree ‚Üí extract text
   - `image/*` ‚Üí AWS Textract or Tesseract OCR
2. Clean extracted text:
   - Remove HTML/RTF formatting artifacts
   - Normalize whitespace
   - Extract sections if structured

**Output**: Plain text content for each document

---

### Agent 4: Clinical Data Extractor (Claude Agent)

**Input**: Plain text documents + patient timeline + variable schema

**Process**:
1. For each document:
   - Classify document type (pathology vs operative vs radiology vs oncology)
   - Extract structured variables based on document type:
     - **Pathology**: diagnosis, WHO grade, tumor location, molecular markers
     - **Operative**: surgery date, type, extent, location
     - **Radiology**: tumor size, enhancement, imaging findings
     - **Oncology**: chemo line, route, dose, clinical status
2. Map extracted values to standardized schema
3. Handle longitudinal variables (multiple surgeries, multiple chemo lines)

**Output**: Structured CSV with all extracted variables

---

## üîß Implementation Pseudocode

```python
def multiagent_binary_extraction(patient_fhir_id):
    # Agent 1: Document Selector
    timeline = extract_patient_timeline(patient_fhir_id)  # From v_procedures_tumor, v_medications
    candidate_docs = query_high_value_documents(patient_fhir_id, timeline)  # From v_binary_files
    ranked_docs = rank_documents_by_value(candidate_docs, timeline)
    top_docs = ranked_docs[:20]  # Top 20 documents

    # Agent 2: S3 Availability Checker
    available_docs = []
    for doc in top_docs:
        if check_s3_availability(doc['binary_id']):
            available_docs.append(doc)

    if len(available_docs) < 15:
        # Trigger fallback selection
        fallback_docs = query_fallback_documents(patient_fhir_id, timeline)
        available_docs.extend(check_availability(fallback_docs))

    # Agent 3: Content Extractor
    extracted_texts = []
    for doc in available_docs:
        raw_content = fetch_from_s3(doc['binary_id'])
        text = extract_text_by_content_type(raw_content, doc['content_type'])
        extracted_texts.append({
            'document_reference_id': doc['document_reference_id'],
            'document_type': doc['dr_type_text'],
            'document_date': doc['dr_date'],
            'text': text
        })

    # Agent 4: Clinical Data Extractor (Claude)
    structured_data = {}
    for doc in extracted_texts:
        variables = claude_extract_clinical_variables(
            document_text=doc['text'],
            document_type=doc['document_type'],
            document_date=doc['document_date'],
            patient_timeline=timeline,
            variable_schema=VARIABLE_SCHEMA
        )
        structured_data.update(variables)

    return structured_data
```

---

## ‚úÖ Next Steps

1. **Update v_binary_files view** ‚úÖ COMPLETED
   - Added all fields from Python extraction query
   - Aggregated multi-valued fields (encounters, type_coding, categories)

2. **Test v_binary_files view in Athena**
   - Execute CREATE VIEW statement
   - Validate query performance
   - Test filtering strategies

3. **Build Document Selector (Agent 1)**
   - Implement multi-field filtering logic
   - Define temporal window rules
   - Create document ranking algorithm

4. **Build S3 Availability Checker (Agent 2)**
   - Implement S3 head_object checks
   - Add fallback selection logic
   - Track availability metrics

5. **Build Content Extractor (Agent 3)**
   - Implement content-type routing
   - Add HTML/RTF/PDF/OCR extractors
   - Test with sample documents

6. **Integrate with Claude (Agent 4)**
   - Design variable extraction prompts
   - Handle longitudinal data
   - Validate against gold standard

---

## üìä Expected Outcomes

### Document Selection Efficiency
- **Before**: 9,348 total documents ‚Üí 40 randomly selected (0.4%)
- **After**: 9,348 total documents ‚Üí 15-20 strategically selected (0.2%)
- **Improvement**: 50% reduction in volume, 2-3√ó increase in variable coverage

### Variable Coverage
- **Structured data only**: 11/35 variables (31%)
- **+ Random 40 documents**: 20/35 variables (57%)
- **+ Targeted 15-20 documents**: 32/35 variables (91%)
- **Missing variables**: Likely require manual chart review

### S3 Availability Impact
- **Attempted**: 20 high-value documents
- **Available (57%)**: ~11-12 documents
- **With fallback**: 15-18 documents
- **Coverage**: Should still achieve 85%+ variable coverage

---

## üéØ Success Metrics

1. **Document Selection Precision**: % of selected documents that contain target variables
   - **Target**: >80% (vs <50% with random selection)

2. **Variable Coverage**: % of 35 variables successfully extracted
   - **Target**: >90% (vs 57% with random documents)

3. **S3 Availability Handling**: % of target documents successfully retrieved
   - **Target**: >75% after fallback logic

4. **Processing Efficiency**: Documents processed per variable extracted
   - **Target**: <2 documents per variable (vs 3-5 with random selection)

5. **Extraction Accuracy**: % agreement with gold standard annotations
   - **Target**: >85% for structured extraction from targeted documents

---

**End of Binary Files Deep Analysis**
